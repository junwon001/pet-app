import os
import sys
from typing import List

try:
    from rag_retriever import retrieve_knowledge
    print("âœ… RAG ê²€ìƒ‰ ëª¨ë“ˆ(rag_retriever) ë¡œë“œ ì„±ê³µ.")
except ImportError:
    print("âŒ ì˜¤ë¥˜: 'rag_retriever.py' íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    sys.exit()


# ============================================
# ğŸš¨ Groq LLaMA ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ê¸°
# ============================================
try:
    from groq import Groq

    if not os.getenv("GROQ_API_KEY"):
        print("\nâš ï¸ ê²½ê³ : GROQ_API_KEY í™˜ê²½ ë³€ìˆ˜ê°€ ì—†ìŠµë‹ˆë‹¤.")
        print("PowerShellì—ì„œ: setx GROQ_API_KEY \"APIí‚¤\"")
        LLM_CLIENT = None
    else:
        LLM_CLIENT = Groq(api_key=os.getenv("GROQ_API_KEY"))
        LLM_MODEL = "llama-3.3-70b-versatile"
        print(f"âœ… LLM ë¡œë“œ ì„±ê³µ: {LLM_MODEL}")

except ImportError:
    print("âŒ 'groq' ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì§€ ì•ŠìŒ. pip install groq í•„ìš”.")
    LLM_CLIENT = None


# ============================================
# ğŸ“Œ í”„ë¡¬í”„íŠ¸ êµ¬ì„± í•¨ìˆ˜
# ============================================
def build_prompt(query: str, contexts: List[str]) -> str:

    if not contexts:
        return (
            f"ì‚¬ìš©ì ì§ˆë¬¸: {query}\n\n"
            "ì°¸ê³ í•  ì§€ì‹ì´ ì—†ìŠµë‹ˆë‹¤. ì¼ë°˜ì ì¸ ì§€ì‹ìœ¼ë¡œ ë‹µë³€í•´ì£¼ì„¸ìš”."
        )

    context_text = "\n---\n".join(contexts)

    system_prompt = (
        "ë‹¹ì‹ ì€ ìˆ˜ì˜í•™ ì „ë¬¸ê°€ AIì…ë‹ˆë‹¤. "
        "ì œê³µëœ ì°¸ê³  ì§€ì‹ë§Œ ì‚¬ìš©í•´ ë‹µë³€í•˜ì„¸ìš”. "
        "ì§€ì‹ì— ì—†ëŠ” ë‚´ìš©ì€ ì¶”ë¡ í•˜ì§€ ë§ê³  'ëª¨ë¥´ê² ë‹¤'ê³  ë§í•˜ì„¸ìš”."
    )

    full_prompt = (
        f"{system_prompt}\n\n"
        f"--- ì°¸ê³  ì§€ì‹ ---\n"
        f"{context_text}\n"
        f"------------------\n\n"
        f"ì‚¬ìš©ì ì§ˆë¬¸: {query}"
    )
    return full_prompt


# ============================================
# ğŸ“Œ ë‹µë³€ ìƒì„± í•¨ìˆ˜
# ============================================
def generate_answer(query: str, filters: dict = None) -> str:

    if LLM_CLIENT is None:
        return "âŒ LLM(API)ê°€ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. API í‚¤ í™˜ê²½ ë³€ìˆ˜ë¥¼ í™•ì¸í•˜ì„¸ìš”."

    print(f"\n[ğŸ”] ì§ˆë¬¸: {query}")

    # 1) Retrieval
    try:
        retrieved_contexts = retrieve_knowledge(query, filters=filters, top_k=5)
    except Exception as e:
        return f"âŒ ê²€ìƒ‰ ì˜¤ë¥˜: {e}"

    # 2) Prompt êµ¬ì„±
    final_prompt = build_prompt(query, retrieved_contexts)
    print(f"[ğŸ’¬] ê²€ìƒ‰ëœ ì»¨í…ìŠ¤íŠ¸ {len(retrieved_contexts)}ê°œ ê¸°ë°˜ìœ¼ë¡œ ë‹µë³€ ìƒì„± ì¤‘...")

    # 3) Groq LLaMA í˜¸ì¶œ
    try:
        response = LLM_CLIENT.chat.completions.create(
            model=LLM_MODEL,
            messages=[
                {"role": "system", "content": "ë„ˆëŠ” ìˆ˜ì˜í•™ ì „ë¬¸ ìƒë‹´ AIì•¼."},
                {"role": "user", "content": final_prompt},
            ],
            max_tokens=512,
            temperature=0.2,
        )

        return response.choices[0].message.content
    
    except Exception as e:
        return f"âŒ LLM API ì˜¤ë¥˜: {e}"


# ============================================
# ğŸš€ í…ŒìŠ¤íŠ¸ ì‹¤í–‰
# ============================================
if __name__ == "__main__":

    q1 = "ìš°ë¦¬ ê°•ì•„ì§€ê°€ ë°¤ì— ê¸°ì¹¨ì„ ë§ì´ í•´. ì™œ ê·¸ëŸ´ê¹Œ?"
    print("\n--- ë‹µë³€ 1 ---")
    print(generate_answer(q1))

    q2 = "ë…¸ë ¹ê²¬ ì¹˜ì£¼ ì§ˆí™˜ ê´€ë¦¬ë²• ì•Œë ¤ì¤˜."
    print("\n--- ë‹µë³€ 2 (ì¹˜ê³¼ í•„í„°) ---")
    print(generate_answer(q2, filters={"department_meta": "ì¹˜ê³¼"}))

    q3 = "ìƒˆë¼ ê°•ì•„ì§€ ì„¤ì‚¬í•  ë•Œ ì§‘ì—ì„œ ë­˜ í•´ì¤„ ìˆ˜ ìˆì–´?"
    print("\n--- ë‹µë³€ 3 (ìƒˆë¼ í•„í„°) ---")
    print(generate_answer(q3, filters={"lifeCycle": "ìƒˆë¼"}))
